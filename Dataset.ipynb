{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM24KKqxeCj3TOvs//TTT36",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aShahawy/GP-Test/blob/main/Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Prep**"
      ],
      "metadata": {
        "id": "Jxm3PXimolPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgeacL95UpOU"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1badu11NqxGf6qM3PTTooQDJvQbejgbTv/view\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1WFl3P9CF23RGhCskLrgm15vEEumvsRWv/view\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1X1EFCyralNN2Bg3LhelL_lShrSrmTitW/view"
      ],
      "metadata": {
        "id": "eVkCb8RBXYPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"CelebAMask-HQ.zip\""
      ],
      "metadata": {
        "id": "2q9hjo0Me_K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"sketch.zip\""
      ],
      "metadata": {
        "id": "7RdDtr_BhWPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"text.zip\""
      ],
      "metadata": {
        "id": "cq4sBe9ZhWvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/sketch/900.jpg /content/174.jpg"
      ],
      "metadata": {
        "id": "1WN2fz28icka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "for i in range(50,100):\n",
        "  # shutil.rmtree(\"/content/\"+str(i)+\"_s.jpg\")\n",
        "  # shutil.rmtree(\"/content/\"+str(i)+\"_o.jpg\")\n",
        "# for i in range(50,100):\n",
        "#   shutil.copy (\"/content/sketch/\"+str(i)+\".jpg\", \"/content/\"+str(i)+\"_s.jpg\")\n",
        "#   shutil.copy (\"/content/CelebAMask-HQ/CelebA-HQ-img/\"+str(i)+\".jpg\", \"/content/\"+str(i)+\"_o.jpg\")"
      ],
      "metadata": {
        "id": "JSN5mYK_Yhgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LO https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhq-1024x1024.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty0iUCOpncOY",
        "outputId": "5a9a2024-d67c-47cf-aecf-6201d5e174ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    93    0    93    0     0    600      0 --:--:-- --:--:-- --:--:--   600\n",
            "100  281M  100  281M    0     0  46.5M      0  0:00:06  0:00:06 --:--:-- 48.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using *StyleGAN*"
      ],
      "metadata": {
        "id": "adwaEzP8n94u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html"
      ],
      "metadata": {
        "id": "evxlJK-TrWXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.20 click==8.0 pillow==8.3.1 scipy==1.7.1 \\\n",
        "requests==2.26.0 tqdm==4.62.2 ninja==1.10.2 matplotlib==3.4.2 imageio==2.9.0 imgui==1.3.0 \\\n",
        "glfw==2.2.0 pyopengl==3.1.5 imageio-ffmpeg==0.4.3 pyspng"
      ],
      "metadata": {
        "id": "0goQnZyPo59g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVlabs/stylegan3\n",
        "!cp /content/stylegan3/dnnlib /content/dnnlib -r\n",
        "!cp /content/stylegan3/torch_utils /content/torch_utils -r\n",
        "!touch /content/stylegan3/no.py\n",
        "# !git clone https://github.com/NVlabs/stylegan3\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVb6Z32_v22J",
        "outputId": "f38517d5-cc50-4ad6-9251-f3e56ea4d6d5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan3'...\n",
            "remote: Enumerating objects: 193, done.\u001b[K\n",
            "remote: Total 193 (delta 0), reused 0 (delta 0), pack-reused 193\u001b[K\n",
            "Receiving objects: 100% (193/193), 4.18 MiB | 30.11 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 195 (delta 12), reused 22 (delta 9), pack-reused 168\u001b[K\n",
            "Receiving objects: 100% (195/195), 8.91 MiB | 22.52 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Obtaining file:///content/CLIP\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.63.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 12.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja, einops\n",
            "Successfully installed einops-0.4.1 ninja-1.10.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip /\n",
        "# torch==1.7.1 \\\n",
        "# tqdm==4.42.1 \\\n",
        "# numpy==1.18.1 \\\n",
        "# requests==2.22.0 \\\n",
        "# pandas==1.0.1 \\\n",
        "# nltk==3.4.5 \\\n",
        "# six==1.14.0 \\\n",
        "# scikit_image==0.16.2 \\\n",
        "# opencv_python==3.4.9.33 \\\n",
        "# torchvision==0.8.2 \\\n",
        "# torch==1.7.1 \\\n",
        "# dominate==2.5.1 \\\n",
        "# matplotlib==3.1.3 \\\n",
        "# visdom==0.1.8.9 \\\n",
        "# pytorch_pretrained_bert==0.6.2 \\\n",
        "# lmdb==1.0.0 \\\n",
        "# scipy==1.4.1 \\\n",
        "# ipython==7.21.0 \\\n",
        "# Pillow==8.1.0 \\\n",
        "# python_dateutil==2.8.1 \\\n",
        "# rawpy==0.16.0 \\\n",
        "# tensorboardX==2.1 \\\n",
        "# streamlit \\\n",
        "# git+https://github.com/openai/CLIP.git \\"
      ],
      "metadata": {
        "id": "xnrd0sOOqQmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "# sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan3')\n",
        "import torch_utils.persistence\n",
        "\n",
        "with open('stylegan3-t-ffhq-1024x1024.pkl', 'rb') as f:\n",
        "    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module\n",
        "z = torch.randn([1, G.z_dim]).cuda()    # latent codes\n",
        "c = None                                # class labels (not used in this example)\n",
        "img = G(z, c)                           # NCHW, float32, dynamic range [-1, +1], no truncation"
      ],
      "metadata": {
        "id": "jnq-p1uPoiqr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.randn([1, G.z_dim]).cuda() \n",
        "img = G(z, c) \n",
        "print(img.shape)\n",
        "# print(G.z_dim)\n",
        "# print(z)\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "tf = Compose([\n",
        "  Resize(224),\n",
        "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "  ])\n",
        "display(TF.to_pil_image(tf(img)[0]))\n"
      ],
      "metadata": {
        "id": "Qsp3dNtttGHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python /content/stylegan3/no.py"
      ],
      "metadata": {
        "id": "jsq-7gGA4M4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import resize\n",
        "from torch.nn.modules.conv import Conv2d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "class CDataset(Dataset):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "        self.simg_dir = \"/content/sketch/\"\n",
        "        self.oimg_dir = \"/content/CelebAMask-HQ/CelebA-HQ-img/\"\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.simg_dir, str(idx+100)+\".jpg\")\n",
        "        simage = read_image(img_path)\n",
        "        img_path = os.path.join(self.oimg_dir, str(idx+100)+\".jpg\")\n",
        "        oimage = read_image(img_path)\n",
        "        if self.transform:\n",
        "            simage = self.transform(simage)\n",
        "            oimage = self.transform(oimage)\n",
        "        return simage, oimage\n",
        "\n",
        "transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                transforms.Resize(512),\n",
        "                                transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "\n",
        "dataset = CDataset(transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "# mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# data_loader = torch.utils.data.DataLoader(dataset=mnist_data,\n",
        "#                                           batch_size=64,\n",
        "#                                           shuffle=True)\n",
        "# dataiter = iter(dataloader)\n",
        "# simg, oimg = dataiter.next()\n",
        "# print(torch.min(images), torch.max(images))\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()        \n",
        "        # N, 3, 512, 512\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, stride=2, padding=1), # -> N, 16, 14, 14\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # -> N, 32, 7, 7\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1), # -> N, 16, 14, 14\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1), # -> N, 32, 7, 7\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1), # -> N, 16, 14, 14\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 512, 3, stride=2, padding=1), # -> N, 16, 14, 14\n",
        "            nn.ReLU(),\n",
        "            # nn.Conv2d(512, 1024, 3, stride=2, padding=1), # -> N, 16, 14, 14\n",
        "            # nn.ReLU(),\n",
        "            # nn.Conv2d(1024, 1563, 3, stride=2, padding=1), # -> N, 16, 14, 14\n",
        "            # nn.ReLU(),\n",
        "            nn.Conv2d(512, 1, 1), # -> N, 64, 1, 1\n",
        "            nn.MaxPool2d(3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        return encoded\n",
        "    \n",
        " \n",
        "# Note: nn.MaxPool2d -> use nn.MaxUnpool2d, or use different kernelsize, stride etc to compensate...\n",
        "# Input [-1, +1] -> use nn.Tanh\n",
        "\n",
        "model = Autoencoder()\n",
        "\n",
        "# criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=1e-3, \n",
        "                             weight_decay=1e-5)\n",
        "dataiter = iter(dataloader)\n",
        "s, o = dataiter.next()\n",
        "ott = model(s)\n",
        "ot = torch.reshape(ott, (1,512))\n",
        "img = G(ot, None)\n",
        "tf = Compose([\n",
        "  Resize(224),\n",
        "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "  ])\n",
        "display(TF.to_pil_image(tf(img)[0]))\n",
        "# Point to training loop video\n",
        "# num_epochs = 1\n",
        "# outputs = []\n",
        "# for epoch in range(num_epochs):\n",
        "#     for (img, _) in data_loader:\n",
        "#         # img = img.reshape(-1, 28*28) # -> use for Autoencoder_Linear\n",
        "#         recon = model(img)\n",
        "#         loss = criterion(recon, img)\n",
        "        \n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
        "#     outputs.append((epoch, img, recon))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "L8mFyMCS_JMT",
        "outputId": "fd77e861-18ab-4a0b-cf95-68eca033e8dd"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-df044c110b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mott\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mott\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m tf = Compose([\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 512]' is invalid for input of size 256"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.reshape(-1, (1,512))\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan3')\n",
        "\n",
        "import io\n",
        "import os, time\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "import copy\n",
        "import imageio\n",
        "import unicodedata\n",
        "import re\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "from google.colab import files\n",
        "from time import perf_counter\n",
        "from stylegan3.dnnlib.util import open_url\n"
      ],
      "metadata": {
        "id": "d6eI-ZSV-Dnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan3')\n",
        "\n",
        "import io\n",
        "import os, time\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "import copy\n",
        "import imageio\n",
        "import unicodedata\n",
        "import re\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "from google.colab import files\n",
        "from time import perf_counter\n",
        "from stylegan3.dnnlib.util import open_url\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "# Load VGG16 feature detector.\n",
        "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
        "with open_url(url) as f:\n",
        "    vgg16 = torch.jit.load(f).eval().to(device)\n",
        "print('Using device:', device, file=sys.stderr)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
        "\n",
        "def norm1(prompt):\n",
        "    \"Normalize to the unit sphere.\"\n",
        "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
        "\n",
        "def embed_image(image):\n",
        "  n = image.shape[0]\n",
        "  cutouts = make_cutouts(image)\n",
        "  embeds = clip_model.embed_cutout(cutouts)\n",
        "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "  return embeds\n",
        "\n",
        "# def embed_url(url):\n",
        "#   image = Image.open(fetch(url)).convert('RGB')\n",
        "#   return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "\n",
        "class CLIP(object):\n",
        "  def __init__(self):\n",
        "    clip_model = \"ViT-B/32\"\n",
        "    self.model, _ = clip.load(clip_model)\n",
        "    self.model = self.model.requires_grad_(False)\n",
        "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_text(self, prompt):\n",
        "      \"Normalized clip text embedding.\"\n",
        "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
        "\n",
        "  def embed_cutout(self, image):\n",
        "      \"Normalized clip image embedding.\"\n",
        "      return norm1(self.model.encode_image(self.normalize(image)))\n",
        "  \n",
        "clip_model = CLIP()\n",
        "\n",
        "# Projector\n",
        "\n",
        "def project(\n",
        "    G,\n",
        "    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
        "    *,\n",
        "    num_steps                  = 1000,\n",
        "    w_avg_samples              = -1,\n",
        "    initial_learning_rate      = 0.1,\n",
        "    initial_noise_factor       = 0.05,\n",
        "    lr_rampdown_length         = 0.25,\n",
        "    lr_rampup_length           = 0.05,\n",
        "    noise_ramp_length          = 0.75,\n",
        "    regularize_noise_weight    = 1e5,\n",
        "    verbose                    = False,\n",
        "    device: torch.device\n",
        "):\n",
        "\n",
        "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
        "\n",
        "    def logprint(*args):\n",
        "        if verbose:\n",
        "            print(*args)\n",
        "\n",
        "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n",
        "\n",
        "    # Compute w stats.\n",
        "    if w_avg_samples > 0:\n",
        "      logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
        "      z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
        "    else:\n",
        "      seed = np.random.randint(0, 2**32 - 1)\n",
        "      z_samples = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
        "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n",
        "    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
        "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
        "\n",
        "    # Setup noise inputs.\n",
        "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
        "\n",
        "    # Features for target image.\n",
        "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
        "    if target_images.shape[2] > 256:\n",
        "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
        "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
        "\n",
        "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
        "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
        "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "\n",
        "    # Init noise.\n",
        "    for buf in noise_bufs.values():\n",
        "        buf[:] = torch.randn_like(buf)\n",
        "        buf.requires_grad = True\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Learning rate schedule.\n",
        "        t = step / num_steps\n",
        "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "        lr = initial_learning_rate * lr_ramp\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Synth images from opt_w.\n",
        "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
        "        synth_images = G.synthesis(ws, noise_mode='const')\n",
        "\n",
        "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
        "        synth_images = (synth_images + 1) * (255/2)\n",
        "        if synth_images.shape[2] > 256:\n",
        "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
        "\n",
        "        # Features for synth images.\n",
        "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
        "        dist = (target_features - synth_features).square().sum()\n",
        "\n",
        "        # Noise regularization.\n",
        "        reg_loss = 0.0\n",
        "        for v in noise_bufs.values():\n",
        "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
        "            while True:\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
        "                if noise.shape[2] <= 8:\n",
        "                    break\n",
        "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "        loss = dist + reg_loss * regularize_noise_weight\n",
        "\n",
        "        # Step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        logprint(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
        "\n",
        "        # Save projected W for each optimization step.\n",
        "        w_out[step] = w_opt.detach()[0]\n",
        "\n",
        "        # Normalize noise.\n",
        "        with torch.no_grad():\n",
        "            for buf in noise_bufs.values():\n",
        "                buf -= buf.mean()\n",
        "                buf *= buf.square().mean().rsqrt()\n",
        "\n",
        "    return w_out.repeat([1, G.mapping.num_ws, 1])\n",
        "\n",
        "def get_perceptual_loss(synth_image, target_features):\n",
        "    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
        "    synth_image = (synth_image + 1) * (255/2)\n",
        "    if synth_image.shape[2] > 256:\n",
        "        synth_image = F.interpolate(synth_image, size=(256, 256), mode='area')\n",
        "\n",
        "    # Features for synth images.\n",
        "    synth_features = vgg16(synth_image, resize_images=False, return_lpips=True)\n",
        "    return (target_features - synth_features).square().sum()\n",
        "\n",
        "def get_target_features(target):\n",
        "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
        "    if target_images.shape[2] > 256:\n",
        "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
        "    return vgg16(target_images, resize_images=False, return_lpips=True)\n",
        "\n",
        "#@markdown #**Model selection** 🎭\n",
        "\n",
        "\n",
        "#@markdown There are 4 pre-trained options to play with:\n",
        "#@markdown - FFHQ: Trained with human faces.\n",
        "#@markdown - MetFaces: Trained with paintings/portraits of human faces.\n",
        "#@markdown - AFHQv2: Trained with animal faces.\n",
        "#@markdown - Cosplay: Trained by [l4rz](https://twitter.com/l4rz) with cosplayer's faces.\n",
        "#@markdown - Wikiart: Trained by [Justin Pinkney](https://www.justinpinkney.com/) with the Wikiart 1024 dataset.\n",
        "#@markdown - Landscapes: Trained by [Justin Pinkney](https://www.justinpinkney.com/) with the LHQ dataset.\n",
        "\n",
        "\n",
        "#@markdown **Run this cell again if you change the model**.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "base_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\"\n",
        "\n",
        "Model = 'FFHQ' #@param [\"FFHQ\", \"MetFaces\", \"AFHQv2\", \"cosplay\", \"Wikiart\", \"Landscapes\"]\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "model_name = {\n",
        "    \"FFHQ\": base_url + \"stylegan3-t-ffhqu-1024x1024.pkl\",\n",
        "    \"MetFaces\": base_url + \"stylegan3-r-metfacesu-1024x1024.pkl\",\n",
        "    \"AFHQv2\": base_url + \"stylegan3-t-afhqv2-512x512.pkl\",\n",
        "    \"cosplay\": \"https://l4rz.net/cosplayface-snapshot-stylegan3t-008000.pkl\",\n",
        "    \"Wikiart\": \"https://drive.google.com/u/0/open?id=18MOpwTMJsl_Z17q-wQVnaRLCUFZYSNkj\",\n",
        "    \"Landscapes\": \"https://drive.google.com/u/0/open?id=14UGDDOusZ9TMb-pOrF0PAjMGVWLSAii1\"\n",
        "}\n",
        "\n",
        "network_url = model_name[Model]\n",
        "\n",
        "with open(fetch_model(network_url), 'rb') as fp:\n",
        "  G = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "w_stds = G.mapping(zs, None).std(0)\n",
        "\n",
        "#@markdown #**Parameters** ✍️\n",
        "#@markdown ---\n",
        "\n",
        "target_image_filename = \"\"#@param {type:\"string\"}\n",
        "text = \"\"#@param {type:\"string\"}\n",
        "loss_ratio = 0.4#@param {type:\"number\"}\n",
        "steps = 800#@param {type:\"number\"}\n",
        "limit_step = 600#@param {type:\"number\"}\n",
        "seed = 14#@param {type:\"number\"}\n",
        "\n",
        "#@markdown Choose -1 for a random seed.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "if seed == -1:\n",
        "    seed = np.random.randint(0,9e9)\n",
        "\n",
        "target = clip_model.embed_text(text)\n",
        "\n",
        "target_pil = Image.open(target_image_filename).convert('RGB')\n",
        "w, h = target_pil.size\n",
        "s = min(w, h)\n",
        "target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
        "target_pil = target_pil.resize((G.img_resolution, G.img_resolution), Image.LANCZOS)\n",
        "target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
        "target_tensor = torch.tensor(target_uint8.transpose([2, 0, 1]), device=device)\n",
        "\n",
        "#@markdown #**Run the model** 🚀\n",
        "\n",
        "# Actually do the run\n",
        "\n",
        "tf = Compose([\n",
        "  Resize(224),\n",
        "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "])\n",
        "\n",
        "\n",
        "def run(timestring, projection_target):\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  target_features = get_target_features(projection_target)\n",
        "\n",
        "  # Init\n",
        "  # Sample 32 inits and choose the one closest to prompt\n",
        "\n",
        "  with torch.no_grad():\n",
        "    qs = []\n",
        "    losses = []\n",
        "    for _ in range(8):\n",
        "      q = (G.mapping(torch.randn([4,G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
        "      images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
        "      loss = get_perceptual_loss(images, target_features)\n",
        "      i = torch.argmin(loss)\n",
        "      qs.append(q[i])\n",
        "      losses.append(loss)\n",
        "    qs = torch.stack(qs)\n",
        "    losses = torch.stack(losses)\n",
        "    i = torch.argmin(losses)\n",
        "    q = qs[i].unsqueeze(0).requires_grad_()\n",
        "\n",
        "  # Sampling loop\n",
        "  q_ema = q\n",
        "  opt = torch.optim.AdamW([q], lr=0.03, betas=(0.0,0.999))\n",
        "  loop = tqdm(range(steps))\n",
        "  for i in loop:\n",
        "    opt.zero_grad()\n",
        "    w = q * w_stds\n",
        "    image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n",
        "    embed = embed_image(image.add(1).div(2))\n",
        "    step_ratio = i / limit_step\n",
        "    perceptual_loss = get_perceptual_loss(image, target_features)\n",
        "    modulated_perceptual_loss = (\n",
        "        max(loss_ratio, 1 - step_ratio)\n",
        "        * get_perceptual_loss(image, target_features)\n",
        "    )\n",
        "    clip_loss = spherical_dist_loss(embed, target).mean()\n",
        "    modulated_clip_loss = (\n",
        "        min(1 - loss_ratio, step_ratio)\n",
        "        * (step_ratio) * spherical_dist_loss(embed, target).mean()\n",
        "    )\n",
        "    loss = modulated_perceptual_loss + modulated_clip_loss\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
        "\n",
        "    q_ema = q_ema * 0.9 + q * 0.1\n",
        "    image = G.synthesis(q_ema * w_stds + G.mapping.w_avg, noise_mode='const')\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      display(TF.to_pil_image(tf(image)[0]))\n",
        "      print(f\"image {i}/{steps} | projector loss: {perceptual_loss} | clip loss: {clip_loss} | modulated loss: {loss}\")\n",
        "    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
        "    os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
        "    pil_image.save(f'samples/{timestring}/{i:04}.jpg')\n",
        "\n",
        "try:\n",
        "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  run(timestring, target_tensor)\n",
        "except KeyboardInterrupt:\n",
        "  pass\n",
        "\n",
        "#@markdown #**Save images** 📷\n",
        "#@markdown A `.tar` file will be saved inside *samples* and automatically downloaded, unless you previously ran the Google Drive cell,\n",
        "#@markdown in which case it'll be saved inside your previously created drive *samples* folder.\n",
        "\n",
        "archive_name = \"optional\"#@param {type:\"string\"}\n",
        "\n",
        "archive_name = slugify(archive_name)\n",
        "\n",
        "if archive_name != \"optional\":\n",
        "  fname = archive_name\n",
        "  # os.rename(f'samples/{timestring}', f'samples/{fname}')\n",
        "else:\n",
        "  fname = timestring\n",
        "# Save images as a tar archive\n",
        "!tar cf samples/{fname}.tar samples/{timestring}\n",
        "if os.path.isdir('drive/MyDrive/samples'):\n",
        "  shutil.copyfile(f'samples/{fname}.tar', f'drive/MyDrive/samples/{fname}.tar')\n",
        "else:\n",
        "  files.download(f'samples/{fname}.tar')\n",
        "\n",
        "#@markdown #**Generate video** 🎥\n",
        "\n",
        "#@markdown You can edit frame rate and stuff by double-clicking this tab.\n",
        "\n",
        "frames = os.listdir(f\"samples/{timestring}\")\n",
        "frames = len(list(filter(lambda filename: filename.endswith(\".jpg\"), frames))) #Get number of jpg generated\n",
        "\n",
        "init_frame = 1 #This is the frame where the video will start\n",
        "last_frame = frames #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "min_fps = 10\n",
        "max_fps = 60\n",
        "\n",
        "total_frames = last_frame-init_frame\n",
        "\n",
        "#Desired video time in seconds\n",
        "video_length = 14 #@param {type:\"number\"}\n",
        "#Video filename\n",
        "video_name = \"\" #@param {type:\"string\"}\n",
        "video_name = slugify(video_name)\n",
        "\n",
        "# frames = []\n",
        "# tqdm.write('Generating video...')\n",
        "# for i in range(init_frame,last_frame): #\n",
        "#     filename = f\"samples/{timestring}/{i:04}.jpg\"\n",
        "#     frames.append(Image.open(filename))\n",
        "\n",
        "fps = np.clip(total_frames/video_length,min_fps,max_fps)\n",
        "\n",
        "!ffmpeg -r {fps} -i samples/{timestring}/%04d.jpg -c:v libx264 -vf fps={fps} -pix_fmt yuv420p samples/{video_name}.mp4 -frames:v {total_frames}\n",
        "\n",
        "# from subprocess import Popen, PIPE\n",
        "# p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', f'samples/{video_name}.mp4'], stdin=PIPE)\n",
        "# for im in tqdm(frames):\n",
        "#     im.save(p.stdin, 'PNG')\n",
        "# p.stdin.close()\n",
        "\n",
        "print(\"The video is now being compressed, wait...\")\n",
        "p.wait()\n",
        "print(\"The video is ready\")\n",
        "\n",
        "#@markdown #**Download video** 📀\n",
        "#@markdown If you're activated the download to GDrive option, the video will be save there. Don't worry about overwritting issues for colliding filenames, an id will be added to them to avoid this.\n",
        "\n",
        "#Video filename\n",
        "to_download_video_name = \"\" #@param {type:\"string\"}\n",
        "to_download_video_name = slugify(to_download_video_name)\n",
        "\n",
        "from google.colab import files\n",
        "if os.path.isdir('drive/MyDrive/samples'):\n",
        "  filelist = glob.glob(f'drive/MyDrive/samples/{to_download_video_name}*.mp4')\n",
        "  video_count = len(filelist)\n",
        "  if video_count:\n",
        "    final_video_name = f\"{to_download_video_name}{video_count}\"\n",
        "  else:\n",
        "    final_video_name = to_download_video_name\n",
        "  shutil.copyfile(f'samples/{video_name}.mp4', f'drive/MyDrive/samples/{final_video_name}.mp4')\n",
        "else:\n",
        "  files.download(f\"{to_download_video_name}.mp4\")"
      ],
      "metadata": {
        "id": "3gm3KjLu6bPi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}